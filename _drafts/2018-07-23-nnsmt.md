---
date: 2018-07-23T07:00:00-07:00
draft: true
title: "Can you train a neural network using an SMT solver?"
excerpt: "Yes, and I did, but you shouldn't."
---

Unless you've been living under a rock of late,
you know that machine learning is reshaping many areas of computer science.
One of my own research areas, [program synthesis][synthpost]---the idea
that we can automatically generate a program
from a specification of what it should do---is not immune.

The similarities between machine learning and program synthesis are striking.
Program synthesis can be viewed as a machine learning problem:
find some parameters (program syntax) for a model (program semantics)
that minimize a loss function (program correctness).
Many of the [most exciting recent results][ap] in program synthesis research
exploit this observation,
applying machine learning techniques
to [augment][deepcoder] and even [replace][np]
traditional synthesis algorithms.
Approaches using machine learning are particularly well suited
for [example-based synthesis][flashfill],
in which the specification is simply a set of
input-output examples the synthesized program should satisfy.

But the similarities run both ways---machine learning can be viewed as a
program synthesis problem, in which we try to fill in some holes (weights)
in a sketch (model) to satisfy a specification (minimal loss).
How can we use program synthesis techniques to improve machine learning?
This direction is criminally underexplored in the literature,
and in this blog post we're going to see why no one works on it.[^why]

## Machine learning using program synthesis

The [synthesis tools I'm most familiar with][buildsynth]
perform program synthesis by solving logical constraints using an [SMT solver][z3].
To do machine learning using program synthesis,
we're going to encode a machine learning problem as a synthesis problem
and solve the resulting logical constraints.

Why would we try this approach when [gradient descent][] and its ilk work so well
for training large machine learning models?
I think there are four potential strengths
that program synthesis brings to the table:

- Synthesis can offer hard **optimality guarantees** (at least over the training set).
  When our synthesis engine returns a trained model, it also guarantees
  that the returned model is the best one possible---there is no other set of weights
  that gives a model more accurate on the training set.
- If we phrase our synthesis problem well,
  we can use it to do **superoptimization** of the trained model---discovering not just
  the best weights in a fixed model (e.g., a neural network) but also altering the *shape* of the model
  to best fit the data. This is in contrast to most traditional learning algorithms,
  which complete the weights for a single fixed topology,
  and rely on an external layer (e.g., grid search) to search for good shapes.
- Building the infrastructure for synthesizing machine learning models
  will automatically give us tools to do **verification** of models.
  We'll be able to prove properties of learned models
  (e.g., that the outputs are always in a reasonable range).
  There's already some research interest in [verifying learned models][reluplex];
  by doing synthesis we get an even richer set of tools.
- A synthesis-based training approach
  will be **declarative**---we won't need to teach the synthesizer anything
  about how to optimize the loss function.
  Instead, we'll simply tell it what the forward pass of a model looks like,
  and what the loss function is,
  and the synthesis engine will figure out how to train it.
  ([Automatic differentiation][autodiff] in some modern machine learning
  frameworks offers a similar benefit, but synthesis takes it to an extreme).

Of course, there are some significant challenges, too.
The most prominent one will be scalability---modern deep learning models
are [gigantic][aicompute] in terms of both training data
(think millions of examples, each with thousands of dimensions)
and model size (millions of weights to be learned).
In contrast, program synthesis research generally deals with programs
on the order of tens or hundreds of operations,
with compact specifications.
That's a pretty big gap to bridge.

### How to train a model using synthesis

We're going to focus on training (fully connected) neural networks in this post,
because they're all the rage right now.
To train a neural network using synthesis,
we implement a *sketch* that simply describes the forward pass
(i.e., compute the output of the network for a given input),
but using *holes* for the weights
(which we'll ask the synthesizer to try to fill in):

![a neural network sketch]({{ "/img/post/nnsmt/cat.png" | absolute_url }}){: width="80%"}

We're going to implement our neural network synthesizer in [Rosette][].
Implementing the forward pass just requires us to describe
the activation of a single neutron---computing the dot product of inputs
and weights, and then applying a [ReLU][] activation function:

{% highlight racket %}
(define (activation inputs weights)
  (define dot (apply + (map * inputs weights)))
  (if (> dot 0) dot 0))
{% endhighlight %}

Now we can compute the activations for an entire layer[^bias]:

{% highlight racket %}
(define (layer inputs weight-matrix)
  (for/list ([weights (in-list weight-matrix)])
    (activation inputs weights)))
{% endhighlight %}

And finally, compute the entire network's output, given its inputs:

{% highlight racket %}
(define (network inputs weights)
  (for/fold ([inputs inputs]) 
            ([weight-matrix (in-list weights)])
    (layer inputs weight-matrix)))
{% endhighlight %}

**Synthesizing XOR.**
The XOR function is the canonical example of
the benefits of a hidden layer in a neural network:
the hidden layer allows the network to learn non-linear functions.
Let's use our simple neural network implementation to synthesize XOR.

First, we need to create a sketch of a desired neural network topology;
for each layer, we create a matrix of unknown (integer) weights of the appropriate size:

{% highlight racket %}
(define (weights-sketch topology)
  (for/list ([prev topology][curr (cdr topology)])
    (for/list ([neuron (in-range curr)])
      (for/list ([input (in-range prev)])
        (define-symbolic* w integer?)
        w))))
{% endhighlight %}

It's well known that a network with a 2-2-1 topology
(i.e., 2 inputs, 2 hidden neurons, 1 output)
is sufficient to learn XOR,
so let's create a sketch of that shape,
and then assert that the network implements XOR:

{% highlight racket %}
(define sketch (weights-sketch '(2 2 1)))
(assert
 (and
  (equal? (network '(0 0) sketch) '(0))
  (equal? (network '(0 1) sketch) '(1))
  (equal? (network '(1 0) sketch) '(1))
  (equal? (network '(1 1) sketch) '(0))))
{% endhighlight %}

Finally, we can ask Rosette to solve this problem:

{% highlight racket %}
(define M (solve #t))
{% endhighlight %}

The result is a model giving values for our weight matrices,
which we can inspect using `evaluate`:

{% highlight racket %}
(evaluate sketch M)
{% endhighlight %}

produces the weights:

    '(((-2 1) (1 -2)) ((1 1)))

or, in visual form:

![an XOR neural network]({{ "/img/post/nnsmt/xor.png" | absolute_url }}){: width="35%" style="min-width: 0"}

We can also use our infrastructure to *prove* properties about neural networks.
For example, we can prove the claim we made above,
that it's not possible to learn a network for XOR without a hidden layer.
By changing the definition of the sketch to exclude the hidden layer:

{% highlight racket %}
(define sketch (weights-sketch '(2 1)))
{% endhighlight %}

and trying the synthesis again, we find `M` is an unsatisfiable solution;
in other words, there is no assignment of (integer) weights to this topology
that correctly implements XOR.

## Training a cat recognizer

Let's move on from XOR to perhaps the most important computer science problem of our time:
recognizing pictures of cats.
Image recognition is going to stress our synthesis-based training pipeline in several ways.
For one, an image is a *much larger input* than a single bit for XOR---each pixel
has three color channels, each of which is an 8-bit integer,
and a single image has thousands or even millions of pixels,
so each training example is complex.
We will also need *many more training examples* than the four we used for XOR.
Finally, we will want to explore *larger topologies* than the simple one for our XOR neural network.

### Optimization and synthesis

The most difficult challenge for training an image classifier will be optimization.
In our XOR example, we were looking for a *perfect* neural network
that was correct on all our training inputs.
For image classification, it's unlikely we'll be able to find such a network.
Instead, we will want to minimize some *loss function*
capturing the classification errors a candidate network makes.
This makes our synthesis problem a quantitative one:
find the solution that minimizes the loss function.

There are sophisticated ways to solve a quantitative synthesis problem,
but in my experience, the following naive solution can be surprisingly effective.
As an example, suppose we want to solve a classic bin-packing problem:
we have five objects with weights *a*, *b*, *c*, *d*, and *e*,
and need to pack as many as possible into a bag without exceeding a weight limit *T*.
We'll create symbolic boolean variables to indicate whether each object is packed,
and define their corresponding weights:

{% highlight racket %}
(define-symbolic* a? b? c? d? e? boolean?)
(define-values (a b c d e) (values 10 40 20 60 25))
{% endhighlight %}

Now we can define a *cost function* telling us the total weight of everything we've packed:

{% highlight racket %}
(define (total-weight a? b? c? d? e?)
  (+ (if a? a 0) (if b? b 0) (if c? c 0) (if d? d 0) (if e? e 0)))
{% endhighlight %}

To find the optimal set of objects to include,
we first find an initial set of objects to include,
and then recursively ask a solver to find a more optimal solution
until it can no longer do so:[^letrec]
{% highlight racket %}
(define T 80)
(define init-sol (solve (assert (< total-weight T))))

(let loop ([sol init-sol])
  (define cost (evaluate total-weight sol))  ; cost of this solution
  (printf "cost: ~v\n" cost)
  (define new-sol (solve (assert (and (< total-weight T)
                                      (> total-weight cost)))))
  (if (sat? new-sol) (loop new-sol) sol))
{% endhighlight %}

Running this example with `T` set to 80 gives us the following output:

    cost: 60
    cost: 70
    cost: 75
    (model
     [a?$0 #t]
     [b?$0 #t]
     [c?$0 #f]
     [d?$0 #f]
     [e?$0 #t])

We found three solutions, of costs 60, 70, and 75.
The optimal solution includes objects *a*, *b*, and *e*,
with a total weight of 75.

### Recognizing cats with metasketches

As part of the evaluation in our [POPL 2016 paper][popl],
we synthesized a neural network that was a simple binary classifier
to recognize cats, using the same optimization technique as above.
We used our *metasketch* abstraction,
introduced in that paper,
to perform a grid search over possible neural network topologies.
Our training data was 40 examples drawn from the [CIFAR-10 dataset][cifar]---20 pictures of cats,
and 20 pictures of airplanes.

![a cat and a plane]({{ "/img/post/nnsmt/catplane.png" | absolute_url }}){: width="40%"}

As if using such a small, low-resolution training set was not enough of a concession to scalability,
we downsampled the training images to 8&times;8 greyscale.

After 35 minutes of training,
our synthesis tool generated a neural network
that achieved 95% accuracy on the training examples.
It also proved that further accuracy improvements on the training set were impossible:
no change to the network topology (up to the bound on the grid search)
or to the weights could improve the training-set accuracy.
The test-set accuracy was much, much worse.

Obviously this result will not revolutionize the field of machine learning.
Our objective in performing this experiment was to demonstrate
that metasketches can solve complex cost functions
(note how the cost function for synthesizing a neural network
involves *executing* the neural network on the training data---it isn't just
a static function of the synthesized program). But can we do better?


## Binary neural networks

Two key reasons for our cat recognizer synthesis tool's poor scalability
are the arithmetic and activation functions involved in a neural network.
Our cat recognizer used 8-bit fixed-point arithmetic and ReLU activations,
but both are expensive for an SMT solver to reason about.

It turns out that these challenges aren't unique to our synthesizer---modern
machine learning research is facing the same issues.
There's a lot of interest in [quantization][] of neural networks,
in which a network's computations are performed at very low precision
to save both storage space and computation time.
The most extreme form of quantization is a [*binary* neural network][bnn],
where weights and activations are each only a single bit!

On the surface, these techniques seem to be a good fit for our synthesis-based training approach.
Smaller weights should make our synthesis more scalable,
allowing us to use bigger networks and more training examples.
To test this hypothesis, we tried to train an [XNOR-Net][xnor]
for the [MNIST handwritten digit][mnist] classification task.
XNOR-Net is a binary neural network design
that replaces the arithmetic for computing activations
(i.e., our `activation` function above)
with efficient bitwise operations.
Our new activation function looks like this,
where `inputs` and `weights` are now bitvectors (i.e., machine integers)
with one bit per element,
rather than lists of numeric elements:

{% highlight racket %}
(define (activation inputs weights)
  (define xnor (bvor (bvand (bvnot inputs) (bvnot weights))
                     (bvand input weights)))
  (popcount xnor))
{% endhighlight %}

The `popcount` function simply counts the number of bits in a bitvector
(returning the result as another bitvector).
This activation function is much more efficient than the dot product we'd usually compute,
since multiplications can be expensive.

### An initial experiment

We synthesized a XNOR-Net classifier from 100 examples drawn from the MNIST dataset,
downsampled to 8&times;8 pixels.
For this experiment, we fixed a 64-32-32-10 neural network topology,
much larger than the cat recognizer above.
Even though we expected the smaller weights to help scalability,
our initial results were not much better than our cat recognizer.
The synthesis tool achieved 100% accuracy on the small training set,
but it took *7 days* to train!
Worse, its accuracy on a test set was an abysmal 15%,
barely better than random when distinguishing 10 digits.

The biggest issue here is that encoding the `popcount` operation in our
activation function is very expensive for an SMT solver.
We have to use one of the various [clever binary tricks][hd]
to encode `popcount`, but they are all expensive and make optimizing
our loss function difficult.
We also use a one-hot encoding for classification results---the network
outputs 10 bits, with each bit corresponding to the prediction for one potential digit.
This one-hot encoding is problematic for our synthesis tool's search;
many possible values of the 10 output bits are invalid
(any value that does not have exactly one bit set),
creating many areas of the search space that are not fruitful.

### Hacking our way to victory

To address the issues with our initial XNOR-Net,
we made a silly hack and a concession.
We replaced the `popcount` in our activation function
with a much more naive operation---we split the *n*-bit
value `xnor` into its upper and lower halves,
and then the output bit checks whether the upper half is greater than the lower half
when the two are interpreted as *n*/2-bit machine integers.
Then we restricted ourselves to training binary classifiers,
trying to distinguish a digit *k* from digits that are not *k*.

For our final experiment,
we upped the training set size to 250 examples,
125 of the target digit *k* and the rest drawn randomly from digits other than *k*.

{% include vega.html data="img/post/nnsmt/xnor_train.js" name="xnor_train" %}

What was expensive? Fixed-point arithmetic is still hard. ReLU is still expensive (c.f. reluplex paper).


Great! Can solve our arithmetic problems by just… not doing any arithmetic. Everything is binary operations instead.

Define a cost function over examples and find optimal solution by iterative deepening. (Pretty common trick!)

Initial example sucked — 15% accuracy on MNIST after 7 days of training time.

Challenges?
- pop count is expensive
- all training examples need to be encoded upfront
- one-hot encoding is inefficient when solving

So instead: different activation function to replace pop counts; parallelize again using meta sketches and boolector, switch to training per-digit classifiers

Works… kinda.

[^why]: I don't work on it either, but part of this work was a [class project][599s] for me.
[^bias]: We're omitting bias terms from our neural network implementation, but they would be easy to add.
[^letrec]: The `(let loop ([a b]) expr ...)` form in this code is defining and immediately invoking a recursive function; it's equivalent to `(letrec ([loop (lambda (a) expr ...)]) (loop b))`.

[synthpost]: https://homes.cs.washington.edu/~bornholt/post/synthesis-explained.html
[deepcoder]: https://arxiv.org/abs/1611.01989
[np]: https://arxiv.org/abs/1511.04834
[flashfill]: https://www.microsoft.com/en-us/research/publication/automating-string-processing-spreadsheets-using-input-output-examples/
[599s]: https://courses.cs.washington.edu/courses/cse599s/18sp/
[z3]: https://github.com/z3prover/z3
[buildsynth]: https://homes.cs.washington.edu/~bornholt/post/building-synthesizer.html
[gradient descent]: https://en.wikipedia.org/wiki/Gradient_descent
[reluplex]: https://arxiv.org/abs/1702.01135
[aicompute]: https://blog.openai.com/ai-and-compute/
[autodiff]: http://www.columbia.edu/~ahd2125/post/2015/12/5/
[ap]: https://alexpolozov.com/blog/program-synthesis-2018/
[rosette]: https://emina.github.io/rosette/
[ReLU]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
[popl]: https://homes.cs.washington.edu/~bornholt/papers/synapse-popl16.pdf
[cifar]: https://www.cs.toronto.edu/~kriz/cifar.html
[quantization]: https://arxiv.org/abs/1609.07061
[bnn]: https://arxiv.org/abs/1602.02830
[xnor]: https://arxiv.org/abs/1603.05279
[mnist]: http://yann.lecun.com/exdb/mnist/
[hd]: http://www.hackersdelight.org/hdcodetxt/pop.c.txt
