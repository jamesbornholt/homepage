---
date: 2018-07-23T07:00:00-07:00
draft: true
title: "Can you train a neural network using an SMT solver?"
excerpt: "Yes, and I did, but you shouldn't."
---

Unless you've been living under a rock of late,
you know that machine learning is reshaping many areas of computer science.
One of my own research areas, [program synthesis][synthpost]---the idea
that we can automatically generate a program
from a specification of what it should do---is not immune.

The similarities between machine learning and program synthesis are striking.
Program synthesis can be viewed as a machine learning problem:
find some parameters (program syntax) for a model (program semantics)
that minimize a loss function (program correctness).
Many of the [most exciting recent results][ap] in program synthesis research
exploit this observation,
applying machine learning techniques
to [augment][deepcoder] and even [replace][np]
traditional synthesis algorithms.
Approaches using machine learning are particularly well suited
for [example-based synthesis][flashfill],
in which the specification is simply a set of
input-output examples the synthesized program should satisfy.

But the similarities run both ways---machine learning can be viewed as a
program synthesis problem, in which we try to fill in some holes (weights)
in a sketch (model) to satisfy a specification (minimal loss).
How can we use program synthesis techniques to improve machine learning?
This direction is criminally underexplored in the literature,
and in this blog post we're going to see why no one works on it.[^why]

## Machine learning using program synthesis

The [synthesis tools I'm most familiar with][buildsynth]
perform program synthesis by solving logical constraints using an [SMT solver][z3].
To do machine learning using program synthesis,
we're going to encode a machine learning problem as a synthesis problem
and solve the resulting logical constraints.

Why would we try this approach when [gradient descent][] and its ilk work so well
for training large machine learning models?
I think there are four potential strengths
that program synthesis brings to the table:

- Synthesis can offer hard **optimality guarantees** (at least over the training set).
  When our synthesis engine returns a trained model, it also guarantees
  that the returned model is the best one possible---there is no other set of weights
  that gives a model more accurate on the training set.
- If we phrase our synthesis problem well,
  we can use it to do **superoptimization** of the trained model---discovering not just
  the best weights in a fixed model (e.g., a neural network) but also altering the *shape* of the model
  to best fit the data. This is in contrast to most traditional learning algorithms,
  which complete the weights for a single fixed topology,
  and rely on an external layer (e.g., grid search) to search for good shapes.
- Building the infrastructure for synthesizing machine learning models
  will automatically give us tools to do **verification** of models.
  We'll be able to prove properties of learned models
  (e.g., that the outputs are always in a reasonable range).
  There's already some research interest in [verifying learned models][reluplex];
  by doing synthesis we get an even richer set of tools.
- A synthesis-based training approach
  will be **declarative**---we won't need to teach the synthesizer anything
  about how to optimize the loss function.
  Instead, we'll simply tell it what the forward pass of a model looks like,
  and what the loss function is,
  and the synthesis engine will figure out how to train it.
  ([Automatic differentiation][autodiff] in some modern machine learning
  frameworks offers a similar benefit, but synthesis takes it to an extreme).

Of course, there are some significant challenges, too.
The most prominent one will be scalability---modern deep learning models
are [gigantic][aicompute] in terms of both training data
(think millions of examples, each with thousands of dimensions)
and model size (millions of weights to be learned).
In contrast, program synthesis research generally deals with programs
on the order of tens or hundreds of operations,
with compact specifications.
That's a pretty big gap to bridge.

### How to train a model using synthesis

We're going to focus on training (fully connected) neural networks in this post,
because they're all the rage right now.
To train a neural network using synthesis,
we implement a *sketch* that simply describes the forward pass
(i.e., compute the output of the network for a given input),
but using *holes* for the weights
(which we'll ask the synthesizer to try to fill in):

![a neural network sketch]({{ "/img/post/nnsmt/cat.png" | absolute_url }}){: width="80%"}

We're going to implement our neural network synthesizer in [Rosette][].
Implementing the forward pass just requires us to describe
the activation of a single neutron---computing the dot product of inputs
and weights, and then applying a [ReLU][] activation function:

{% highlight racket %}
(define (activation inputs weights)
  (define dot (apply + (map * inputs weights)))
  (if (> dot 0) dot 0))
{% endhighlight %}

Now we can compute the activations for an entire layer[^bias]:

{% highlight racket %}
(define (layer inputs weight-matrix)
  (for/list ([weights (in-list weight-matrix)])
    (activation inputs weights)))
{% endhighlight %}

And finally, compute the entire network's output, given its inputs:

{% highlight racket %}
(define (network inputs weights)
  (for/fold ([inputs inputs]) 
            ([weight-matrix (in-list weights)])
    (layer inputs weight-matrix)))
{% endhighlight %}

**Synthesizing XOR.**
The XOR function is the canonical example of
the benefits of a hidden layer in a neural network:
the hidden layer allows the network to learn non-linear functions.
Let's use our simple neural network implementation to synthesize XOR.

First, we need to create a sketch of a desired neural network topology;
for each layer, we create a matrix of unknown (integer) weights of the appropriate size:

{% highlight racket %}
(define (weights-sketch topology)
  (for/list ([prev topology][curr (cdr topology)])
    (for/list ([neuron (in-range curr)])
      (for/list ([input (in-range prev)])
        (define-symbolic* w integer?)
        w))))
{% endhighlight %}

It's well known that a network with a 2-2-1 topology
(i.e., 2 inputs, 2 hidden neurons, 1 output)
is sufficient to learn XOR,
so let's create a sketch of that shape,
and then assert that the network implements XOR:

{% highlight racket %}
(define sketch (weights-sketch '(2 2 1)))
(assert
 (and
  (equal? (network '(0 0) sketch) '(0))
  (equal? (network '(0 1) sketch) '(1))
  (equal? (network '(1 0) sketch) '(1))
  (equal? (network '(1 1) sketch) '(0))))
{% endhighlight %}

Finally, we can ask Rosette to solve this problem:

{% highlight racket %}
(define M (solve #t))
{% endhighlight %}

The result is a model giving values for our weight matrices,
which we can inspect using `evaluate`:

{% highlight racket %}
(evaluate sketch M)
{% endhighlight %}

produces the weights:

    '(((-2 1) (1 -2)) ((1 1)))

or, in visual form:

![an XOR neural network]({{ "/img/post/nnsmt/xor.png" | absolute_url }}){: width="35%" style="min-width: 0"}

We can also use our infrastructure to *prove* properties about neural networks.
For example, we can prove the claim we made above,
that it's not possible to learn a network for XOR without a hidden layer.
By changing the definition of the sketch to exclude the hidden layer:

{% highlight racket %}
(define sketch (weights-sketch '(2 1)))
{% endhighlight %}

and trying the synthesis again, we find `M` is an unsatisfiable solution;
in other words, there is no assignment of weights to this topology
that correctly implements XOR.

## Training a cat recognizer
Did this in our POPL paper. Took two classes from CIFAR-10(?) (cats, airplanes). Trained a binary classifier. Metasketches did grid search over possible topologies.

Difficulties: can’t expect 100% accuracy on training data (overfitting), so need to do cost function optimization. 

Lots of caveats here: only evaluated on the training set (and only 95%) accuracy. Took 35 minutes to train on 40 examples.

What was expensive? Fixed-point arithmetic is still hard. ReLU is still expensive (c.f. reluplex paper).

## Binary neural networks
Great! Can solve our arithmetic problems by just… not doing any arithmetic. Everything is binary operations instead.

Define a cost function over examples and find optimal solution by iterative deepening. (Pretty common trick!)

Initial example sucked — 15% accuracy on MNIST after 7 days of training time.

Challenges?
- pop count is expensive
- all training examples need to be encoded upfront
- one-hot encoding is inefficient when solving

So instead: different activation function to replace pop counts; parallelize again using meta sketches and boolector, switch to training per-digit classifiers

Works… kinda.

[^why]: I don't work on it either, but part of this work was a [class project][599s] for me.
[^bias]: We're omitting a bias from our neural network implementation, but it would be easy to add.

[synthpost]: https://homes.cs.washington.edu/~bornholt/post/synthesis-explained.html
[deepcoder]: https://arxiv.org/abs/1611.01989
[np]: https://arxiv.org/abs/1511.04834
[flashfill]: https://www.microsoft.com/en-us/research/publication/automating-string-processing-spreadsheets-using-input-output-examples/
[599s]: https://courses.cs.washington.edu/courses/cse599s/18sp/
[z3]: https://github.com/z3prover/z3
[buildsynth]: https://homes.cs.washington.edu/~bornholt/post/building-synthesizer.html
[gradient descent]: https://en.wikipedia.org/wiki/Gradient_descent
[reluplex]: https://arxiv.org/abs/1702.01135
[aicompute]: https://blog.openai.com/ai-and-compute/
[autodiff]: http://www.columbia.edu/~ahd2125/post/2015/12/5/
[ap]: https://alexpolozov.com/blog/program-synthesis-2018/
[rosette]: https://emina.github.io/rosette/
[ReLU]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
