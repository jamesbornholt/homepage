---
date: 2018-07-23T07:00:00-07:00
draft: true
title: "Can you train a neural network using an SMT solver?"
excerpt: "Yes, and I did, but you shouldn't."
---

Unless you've been living under a rock of late,
you know that machine learning is reshaping many areas of computer science.
One of my own research interests, [program synthesis][synthpost]---the idea
that we can automatically generate a program
from a specification of what it should do---is not immune.


The similarities between machine learning and program synthesis are striking.
Program synthesis can be viewed as a machine learning problem:
find some parameters (program syntax) to a model (program semantics)
that minimize a loss function (program correctness).
Many of the most exciting recent results in program synthesis research
exploit this observation,
applying machine learning techniques
to [augment][deepcoder] and even [replace][np]
traditional synthesis algorithms.
Approaches using machine learning are particularly well suited
for [example-based synthesis][flashfill],
in which the specification is simply a set of
input-output examples the synthesized program should satisfy.

But the similarities run both ways---machine learning can be viewed as a
program synthesis problem, in which we try to fill in some holes (weights)
in a sketch (model) to satisfy a specification (minimal loss).
How can we use program synthesis techniques to improve machine learning?
This direction is criminally underexplored in the literature,
and in this blog post we're going to see why no one works on it.[^why]

## Machine learning using program synthesis

The [synthesis tools I'm most familiar with][buildsynth]
perform program synthesis by solving logical constraints using an [SMT solver][z3].
To do machine learning using program synthesis,
we're therefore going to encode a machine learning problem as a synthesis problem
and solve the resulting logical constraints.

TK

Why?? Optimality guarantees (over training set). Superoptimization. Verification (the same infra we need for synthesis can be reused to do verification)

How?? Weights are just holes; we’ll try to find values that satisfy a set of IO examples (i.e., training data). Use SMT solver to solve the synthesis query.

Difficulties: can’t expect 100% accuracy on training data (overfitting), so need to do cost function optimization. 

## Training a cat recognizer
Did this in our POPL paper. Took two classes from CIFAR-10(?) (cats, airplanes). Trained a binary classifier. Metasketches did grid search over possible topologies.

Lots of caveats here: only evaluated on the training set (and only 95%) accuracy. Took 35 minutes to train on 40 examples.

What was expensive? Fixed-point arithmetic is still hard. ReLU is still expensive (c.f. reluplex paper).

## Binary neural networks
Great! Can solve our arithmetic problems by just… not doing any arithmetic. Everything is binary operations instead.

Define a cost function over examples and find optimal solution by iterative deepening. (Pretty common trick!)

Initial example sucked — 15% accuracy on MNIST after 7 days of training time.

Challenges?
- pop count is expensive
- all training examples need to be encoded upfront
- one-hot encoding is inefficient when solving

So instead: different activation function to replace pop counts; parallelize again using meta sketches and boolector, switch to training per-digit classifiers

Works… kinda.

[^why]: I don't work on it either, but part of this work was a [class project][599s] for me.

[synthpost]: https://homes.cs.washington.edu/~bornholt/post/synthesis-explained.html
[deepcoder]: https://arxiv.org/abs/1611.01989
[np]: https://arxiv.org/abs/1511.04834
[flashfill]: https://www.microsoft.com/en-us/research/publication/automating-string-processing-spreadsheets-using-input-output-examples/
[599s]: https://courses.cs.washington.edu/courses/cse599s/18sp/
[z3]: https://github.com/z3prover/z3
[buildsynth]: https://homes.cs.washington.edu/~bornholt/post/building-synthesizer.html

